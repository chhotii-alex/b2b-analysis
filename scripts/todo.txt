____ Use Jasper's Pymmunomics _____

* In count_genes: currently n is fixed; set n to 2*(# available disease patients)
* In count_genes: for each cohort patient, combine data from all their samples (this is correct for cancer patients, not sure what to do about infection)

--- yeah  --

* any weird, rare V genes?

* greylock: compare results; refactor greylock so I can use greylock

* adaptive max load in make_similarity?

* argparse:
- number of genes per sample to truncate to
- chain type
- k for kmers (with the sparseness code, k > 3 tractable? test that)
- max length diff for nonzero similarity

* some V genes in same clade? parse clade? more similar within clade? parameterize how much to clump?

* For a given type of gene (for example, functional IGH) do not use all the sequences; use only the top N (ranked by UMI_family_size). I think this is desirable, lest there
be artifact. I'm doing this now; however, two issues:
1) What should N be?
2) What to do with samples with fewer than N sequences of that type?
These issues are somewhat in tension; if I chose a small N, more of the data is thrown out; however if I chose a large N, more samples have fewer than that number of sequences.
How much is the number of distinct sequence + UMI pairs read a function of how many B cells a patient has, and how much is it a function of processing artifacts?
a) Look at the actual distributions of numbers of genes read per sample
b) Discuss with Ramy

* OOP some separations of concerns

* everything in little pieces suitable for nextflow workflow for re-startability

* matching samples by sex, age, batch, etc.

* Look at Pattern Classification book. What was in there-- trie?

* Look at Hoissein's idea; look at Reeve; meaning of the off-diagonals (comm1 vs. comm2)?

* Box cli for obtaining data (separate project)
